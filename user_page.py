import streamlit as st
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import Ollama
from langchain.embeddings import OllamaEmbeddings

# Ollama é…ç½®
OLLAMA_BASE_URL = 'http://192.168.124.18:11434'  # Ollama çš„ IP å’Œç«¯å£
LLM_MODEL_NAME = 'qwen2'  # LLM æ¨¡å‹åç§°
EMBEDDING_MODEL_NAME = 'mxbai-embed-large'  # åµŒå…¥æ¨¡å‹åç§°

# åˆå§‹åŒ– OllamaEmbeddings
EMBEDDINGS = OllamaEmbeddings(
    model=EMBEDDING_MODEL_NAME,
    base_url=OLLAMA_BASE_URL
)

# æŒä¹…åŒ–å­˜å‚¨ç›®å½•
PERSIST_DIRECTORY = 'db'

def user_page():
    st.title('ğŸ’¬ ç”¨æˆ·ç•Œé¢')
    st.write('ä¸ AI è¿›è¡ŒèŠå¤©ã€‚')

    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
    vectorstore = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=EMBEDDINGS
    )

    if vectorstore._collection.count() == 0:
        st.warning('çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆåœ¨ç®¡ç†å‘˜ç•Œé¢ä¸Šä¼ æ–‡ä»¶ã€‚')
        return

    # åˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œå¢åŠ æ£€ç´¢å‚æ•°
    retriever = vectorstore.as_retriever(search_kwargs={'k': 5})  # è°ƒæ•´ k å€¼ï¼Œå¢åŠ è¿”å›çš„æ–‡æ¡£æ•°é‡

    # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆOllamaï¼‰
    llm = Ollama(
        model=LLM_MODEL_NAME,
        base_url=OLLAMA_BASE_URL
    )

    # åˆ›å»º ConversationalRetrievalChainï¼Œå¹¶è®¾ç½® return_source_documents=True
    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True
    )

    # èŠå¤©å†å²è®°å½•
    if 'chat_history' not in st.session_state:
        st.session_state['chat_history'] = []

    # ç”¨æˆ·è¾“å…¥
    query = st.text_input('è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š')

    if query:
        with st.spinner('AI æ­£åœ¨æ€è€ƒ...'):
            result = qa({'question': query, 'chat_history': st.session_state['chat_history']})
            st.session_state['chat_history'].append((query, result['answer']))

            st.markdown(f"**ç”¨æˆ·**ï¼š{query}")
            st.markdown(f"**AI**ï¼š{result['answer']}")

            # æ˜¾ç¤ºå¼•ç”¨çš„æºæ–‡æ¡£å†…å®¹
            if 'source_documents' in result:
                st.subheader('ğŸ” å¼•ç”¨çš„æºæ–‡æ¡£å†…å®¹ï¼š')
                for idx, doc in enumerate(result['source_documents']):
                    st.markdown(f"**æ–‡æ¡£ {idx + 1}**")
                    st.markdown(f"*å†…å®¹ï¼š* {doc.page_content}")
                    if 'source' in doc.metadata:
                        st.markdown(f"*æ¥æºï¼š* {doc.metadata['source']}")
                    st.markdown("---")

    # æ˜¾ç¤ºèŠå¤©è®°å½•
    if st.session_state['chat_history']:
        st.subheader('ğŸ“ èŠå¤©è®°å½•')
        for i, (user_q, ai_a) in enumerate(st.session_state['chat_history']):
            st.markdown(f"**ç”¨æˆ·**ï¼š{user_q}")
            st.markdown(f"**AI**ï¼š{ai_a}")
